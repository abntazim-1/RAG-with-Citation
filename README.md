ğŸ§  RAG Chatbot with Citation

A Retrieval-Augmented Generation (RAG) based chatbot that can answer user queries using local documents, while also providing source citations for transparency.
This project integrates LlamaIndex, Ollama LLM, and Hugging Face Embeddings to build a self-contained, offline-capable document question-answering system with a simple and interactive interface.

ğŸš€ Features

ğŸ“„ Document-Aware Responses â€“ Answers are grounded in uploaded files.

ğŸ“š Citation Support â€“ Each response includes the filename sources.

âš¡ Fast Local Inference â€“ Runs fully offline using Ollama models (e.g., llama3.2:latest).

ğŸ§© RAG Pipeline â€“ Built on top of LlamaIndex for modularity and extendability.

ğŸ’» Simple Web Interface â€“ Clean frontend for chatting with your documents.

ğŸ—ï¸ Project Structure
RAG-Chatbot-with-Citation/
â”‚
â”œâ”€â”€ data/                     # Folder containing user-uploaded or reference documents
â”‚   â”œâ”€â”€ doc1.pdf
â”‚   â””â”€â”€ notes.txt
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ app.py                # Flask-based backend API for RAG responses
â”‚
â”œâ”€â”€ rag_pipeline.py            # Main pipeline defining embeddings, LLM, and query engine
â”‚
â”œâ”€â”€ run_app.py                 # Runs both backend + frontend interface
â”‚
â”œâ”€â”€ requirements.txt           # Required Python dependencies
â”‚
â””â”€â”€ README.md                  # Documentation (this file)

âš™ï¸ Setup Instructions
1. Clone the Repository
git clone https://github.com/<your-username>/RAG-Chatbot-with-Citation.git
cd RAG-Chatbot-with-Citation

2. Create and Activate a Virtual Environment
python -m venv venv
# Activate the environment
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

3. Install Dependencies
pip install -r requirements.txt

4. Install Ollama

Download and install Ollama from https://ollama.com/download

Once installed, pull your model (example: llama3.2):

ollama pull llama3.2

5. Prepare Your Documents

Place any .txt, .pdf, or .md files inside the /data folder.
These documents will serve as your knowledge base.

6. Run the App
python run_app.py


Your chatbot will be available at:

http://127.0.0.1:7860

ğŸ§  RAG Architecture Overview

Retrieval-Augmented Generation (RAG) combines information retrieval with language generation:

Document Ingestion â€“ Local documents are loaded and preprocessed.

Embedding Generation â€“ Each document chunk is converted into a dense vector using sentence-transformers/all-MiniLM-L6-v2.

Vector Indexing â€“ The vectors are stored in a VectorStoreIndex (from LlamaIndex).

Retrieval â€“ For each user query, the top relevant document chunks are retrieved using similarity search.

Augmented Generation â€“ The retrieved text is combined with the user query and passed to the LLM (Ollama) for response generation.

Citation Mapping â€“ The final output displays the sources used for the answer.

ğŸ§© Core Components

LlamaIndex â€“ Manages document indexing and retrieval.

Langchain Embedding (HuggingFace) â€“ Creates embeddings for semantic similarity.

Ollama LLM â€“ Runs a local large language model (like llama3.2).

Flask â€“ Lightweight backend for the API endpoints.

Gradio Interface â€“ Interactive web UI for chatting and viewing citations.

ğŸ§ª Example Usage

Query:

â€œWhat is the purpose of reinforcement learning?â€

Response:

Reinforcement learning focuses on training agents to make sequences of decisions by rewarding good behavior and penalizing poor choices.

Sources:

ğŸ“˜ machine_learning_notes.txt
ğŸ“˜ AI_research_paper.pdf

ğŸ› ï¸ Customization

You can change the embedding model in rag_pipeline.py:

HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")


Swap the LLM model to another Ollama model:

Ollama(model="mistral:latest")


Adjust retrieval depth:

query_engine = index.as_query_engine(similarity_top_k=5)

ğŸ¤– Future Enhancements

âœ… Multi-user session tracking

âœ… Chat history with memory persistence

âœ… Improved citation ranking

âœ… UI enhancements for better document management

ğŸ‘¨â€ğŸ’» Author

Abdullah Bin Noor Tazim
AI Engineer | Machine Learning Enthusiast
