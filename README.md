ğŸ§  RAG Chatbot with Citation

A Retrieval-Augmented Generation (RAG) based chatbot that can answer user queries using local documents, while also providing source citations for transparency.
This project integrates LlamaIndex, Ollama LLM, and Hugging Face Embeddings to build a self-contained, offline-capable document question-answering system with a simple and interactive interface.

ğŸš€ Features

ğŸ“„ Document-Aware Responses â€“ Answers are grounded in uploaded files.

ğŸ“š Citation Support â€“ Each response includes the filename sources.

âš¡ Fast Local Inference â€“ Runs fully offline using Ollama models (e.g., llama3.2:latest).

ğŸ§© RAG Pipeline â€“ Built on top of LlamaIndex for modularity and extendability.

ğŸ’» Simple Web Interface â€“ Clean frontend for chatting with your documents.

ğŸ—ï¸ Project Structure
```
RAG-Chatbot-with-Citation/
â”‚
â”œâ”€â”€ data/                          # Folder containing user-uploaded or reference documents
â”‚   â”œâ”€â”€ sample_policies/           # Default document directory
â”‚   â”‚   â”œâ”€â”€ *.pdf                 # PDF documents
â”‚   â”‚   â””â”€â”€ *.txt                  # Text documents
â”‚   â””â”€â”€ vector_store/              # Vector store cache (auto-generated)
â”‚
â”œâ”€â”€ src/                           # Core source code modules
â”‚   â”œâ”€â”€ rag_pipeline.py           # Main RAG pipeline orchestrator
â”‚   â”œâ”€â”€ data_loader.py             # Document loading and chunking
â”‚   â”œâ”€â”€ embedding.py               # Embedding model initialization
â”‚   â”œâ”€â”€ llm_setup.py               # LLM (Ollama) configuration
â”‚   â”œâ”€â”€ query_engine.py             # Query engine and reranking
â”‚   â””â”€â”€ utils.py                   # Logging and utility functions
â”‚
â”œâ”€â”€ logs/                          # Application logs
â”‚   â”œâ”€â”€ app.log                    # Main application log
â”‚   â””â”€â”€ run_app.log                # Runtime log
â”‚
â”œâ”€â”€ run_app.py                     # Main entry point (Streamlit interface)
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This documentation
```

âš™ï¸ Setup Instructions
1. Clone the Repository
git clone https://github.com/<your-username>/RAG-Chatbot-with-Citation.git
cd RAG-Chatbot-with-Citation

2. Create and Activate a Virtual Environment
python -m venv venv
# Activate the environment
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

3. Install Dependencies
pip install -r requirements.txt

4. Install Ollama

Download and install Ollama from https://ollama.com/download

Once installed, pull your model (example: llama3.2):

ollama pull llama3.2

5. Prepare Your Documents

Place any .txt, .pdf, or .md files inside the /data folder.
These documents will serve as your knowledge base.

6. Run the App

Start the Streamlit application:
```bash
streamlit run run_app.py
```

Or use Python directly:
```bash
python run_app.py
```

Your chatbot will be available at:

**http://127.0.0.1:8501** (Streamlit default port)

The application will automatically:
- Load documents from `data/sample_policies/`
- Initialize the embedding model and vector index
- Start the Ollama LLM connection
- Launch the interactive web interface

**Note:** Make sure Ollama is running before starting the app. You can verify by running `ollama list` in a separate terminal.

ğŸ§  RAG Architecture Overview

Retrieval-Augmented Generation (RAG) combines information retrieval with language generation:

Document Ingestion â€“ Local documents are loaded and preprocessed.

Embedding Generation â€“ Each document chunk is converted into a dense vector using `intfloat/e5-large-v2` (configurable in `src/embedding.py`).

Vector Indexing â€“ The vectors are stored in a VectorStoreIndex (from LlamaIndex).

Retrieval â€“ For each user query, the top relevant document chunks are retrieved using similarity search.

Augmented Generation â€“ The retrieved text is combined with the user query and passed to the LLM (Ollama) for response generation.

Citation Mapping â€“ The final output displays the sources used for the answer.

ğŸ§© Core Components

- **LlamaIndex** â€“ Manages document indexing, vector storage, and retrieval
- **Langchain Embedding (HuggingFace)** â€“ Creates embeddings for semantic similarity using `intfloat/e5-large-v2`
- **Ollama LLM** â€“ Runs a local large language model (default: `llama3.2:1b`)
- **CrossEncoder Reranker** â€“ Optional reranking using `cross-encoder/ms-marco-MiniLM-L-6-v2` for improved relevance
- **Streamlit** â€“ Interactive web UI for chatting and viewing citations
- **Session-based Chat History** â€“ Each user session maintains its own conversation history

ğŸ§ª Example Usage

Query:

â€œWhat is the purpose of reinforcement learning?â€

Response:

Reinforcement learning focuses on training agents to make sequences of decisions by rewarding good behavior and penalizing poor choices.

Sources:

ğŸ“˜ machine_learning_notes.txt
ğŸ“˜ AI_research_paper.pdf

ğŸ› ï¸ Customization

**Change the Embedding Model**

Edit `src/embedding.py` or pass parameters to `RAGPipeline`:
```python
rag = RAGPipeline(
    docs_folder="data/sample_policies",
    embedding_model="sentence-transformers/all-mpnet-base-v2"  # Change here
)
```

**Swap the LLM Model**

Edit `src/llm_setup.py` or pass parameters:
```python
rag = RAGPipeline(
    docs_folder="data/sample_policies",
    llm_model="mistral:latest"  # Change here
)
```

**Adjust Retrieval Parameters**

Edit `src/rag_pipeline.py`:
```python
result = rag_pipeline.ask(query, top_k=10)  # Retrieve more documents
```

**Disable Reranking**

For faster responses (slightly lower quality):
```python
rag = RAGPipeline(
    docs_folder="data/sample_policies",
    use_rerank=False
)
```

ğŸ¤– Features & Notes

âœ… **Per-Session Chat History** â€“ Each Streamlit session maintains its own conversation history, isolated from other users

âœ… **Citation Support** â€“ Every response includes source document citations with page numbers when available

âœ… **Reranking** â€“ Optional CrossEncoder reranking improves answer relevance

âœ… **Offline Operation** â€“ Fully local inference, no external API calls required

âœ… **Modular Architecture** â€“ Clean separation of concerns for easy customization

**Future Enhancements**

- [ ] Persistent chat history across sessions (database storage)
- [ ] Multi-user authentication and session management
- [ ] Document upload interface
- [ ] Export chat history to PDF/JSON
- [ ] Advanced citation ranking algorithms
- [ ] Support for more document formats (Word, Excel, etc.)

ğŸ‘¨â€ğŸ’» Author

Abdullah Bin Noor Tazim
AI Engineer | Machine Learning Enthusiast
